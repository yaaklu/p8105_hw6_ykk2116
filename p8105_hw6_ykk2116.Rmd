---
title: "P8105_hw6_ykk2116"
author: "Yaa Klu"
date: "11/26/2018"
output: github_document
---

**Loading packages**

```{r}
library(tidyverse)
library(modelr)
```


### Problem 1

Reading of data and data wrangling
The variable city_state depicts, cities and the states they are in. for example, New York, NY. The victim's race was recategorized into non-white and whites (reference).
Victim's age was also changed into a numeric variable.
Lastly, an indication of whethwe a homicide is solved or not was created.


```{r}
homicide = read_csv("https://raw.githubusercontent.com/washingtonpost/data-homicides/master/homicide-data.csv", col_names = TRUE) %>% 
  mutate(city_state = str_c(city, ",", " ", state),
         solved = if_else(disposition == "Closed by arrest", "resolved", "unresolved"),
         solved = fct_relevel(solved, "unresolved"),
         victim_race = tolower(victim_race),
         colpsd_victim_race = fct_collapse(victim_race, "non-white" = c("asian","black", "hispanic", "other", "unknown")),
         colpsd_victim_race = fct_relevel(colpsd_victim_race, "white"),
         victim_age = as.numeric(victim_age)) %>% 
  filter(!(city_state %in% c("Dallas, TX", "Phoenix, AZ", "Kansas City, MO", "Tulsa, AL")))
```



The glm function was used to fit a logistic regression model using the victim's age, race and sex as predictor variables and the outcome variable was: resolved or unresolved homicide.

The estimates and confidence intervals were also obtained.

```{r}
baltimore = homicide %>% 
  filter(city_state == "Baltimore, MD")

baltimore_logistic = glm(solved ~ victim_age + victim_sex + colpsd_victim_race, data = baltimore, family = binomial())

```



Table

```{r}

baltimore_logistic %>% broom::tidy() %>% 
  janitor::clean_names() %>% 
  mutate(OR = exp(estimate),
         lower_95_ci = exp(estimate - (1.96 * std_error)),
         upper_95_ci = exp(estimate + (1.96 * std_error))) %>% 
  filter(term == "colpsd_victim_racenon-white") %>% 
  select(OR, lower_95_ci, upper_95_ci) %>% 
  knitr::kable(digits = 2)


```

From the table above, the odds of a solved homicide case, comparing whites to non-whites (adjusted for age and sex) is 0.44 with a 95% confidence interval of (0.31, 0.62)


Running similar glms (as above) for each city.

```{r}
cities_logistics = homicide %>% 
  #Selecting only the variables needed
  select(city_state, solved, victim_age, victim_sex, colpsd_victim_race) %>%
  #Making listcolumns to use for the iteration
  group_by(city_state) %>% 
  nest() %>% 
  #Using maps to iterate the glm and tidy functions
  mutate(models = map(.x = data, ~ glm(solved ~ victim_sex + colpsd_victim_race + victim_age, 
                                      family = binomial, data = .x)),
         models = map(models, broom::tidy)) %>% 
  select(-data) %>% unnest() %>% 
  filter(term == "colpsd_victim_racenon-white") %>% 
  mutate(OR = exp(estimate),
  # Calculating the 95% confidence intervals
         lower_95_ci = exp(estimate - (1.96*std.error)),
         upper_95_ci = exp(estimate + (1.96*std.error))) %>% 
  select(city_state, OR, lower_95_ci, upper_95_ci) %>% 
  #Organizing cities according to estimated OR. From lowest to highest
  mutate(city_state = reorder(city_state, OR))
```



A graph that shows the estimated odds ratios and confidence intervals for each city

```{r}
ggplot(cities_logistics, aes(x = city_state, y = OR )) + 
  geom_point() + 
  geom_errorbar(aes(ymin = lower_95_ci, ymax = upper_95_ci)) + 
  coord_flip() +
  geom_hline(aes(yintercept = 1.00), linetype = "dashed", color = "red") + 
  labs(
    y = "OR (95% CI)",
    x = "City, State"
  )
```

The graph above shows that the odds ratios for most of the cities were below 1.0. However, for Durham, NC is looked as if it was 1.0 and for Tampa, Fl and Brimingham, AL, the odds ratios were greater than 1.0. The confidence intervals were wide for most of them and contained 1.0, indicating statistical isginficance. Regardless, it shows that in most of the cities, it is highly likely that the odds of a homicide being solved is lower for victims who were not Whites, compared to those who were Whites.



### Problem 2

Loading in dataset and data wrangling.

```{r}
child_df = read_csv("./data/birthweight.csv") %>% 
  mutate(babysex = as.factor(babysex),
         frace = as.factor(frace),
         mrace = as.factor(mrace),
         malform = as.factor(malform))
filter_all(child_df, any_vars(is.na(.))) 
sum(is.na(child_df))
```

After data wrangling we see that there is not missing observations. 
The outcome of interest here is birth weight, which is a continuous variable.
I choose a linear regression for this analysis because of the continuous variable.

From prior knowledge (literature), I would like to look at a linear regression model using birth weight as an outcome and gestational age as a predictor. Birthweight is usually associated with gestational age. In the same model, I would adjust for the following variables including; baby's sex, mother's age at delivry, mother's race, father's race, mother's weight gain during pregnancy, and average number of cigarette smoked per day during pregnancy. 


Distribution of the outcome and predictor variables

Gestational age
```{r}
ggplot(child_df, aes(x = gaweeks)) + geom_histogram()
```


Birth weight
```{r}
ggplot(child_df, aes(x = bwt)) + geom_histogram()
```

Birth weight looks like a normal distribution whiles gestational age shows somewhat a left skew.



Fitting linear model

```{r}
proposed_fit = lm(bwt ~ gaweeks + babysex + momage + mrace + frace + wtgain +  smoken, data = child_df)

proposed_fit %>% 
  broom::tidy() 
```



```{r}
child_df %>% 
  modelr::add_predictions(proposed_fit) %>% 
  modelr::add_residuals(proposed_fit) %>% 
  ggplot(aes(x = pred, y = resid)) + geom_point() +
  labs(x = "Predicted value", 
       y = "Residual")
```


Two other models

```{r}
main_effect_fit = lm(bwt ~ blength + gaweeks, data = child_df)

interation_fit = lm(bwt ~ bhead * blength *babysex, data = child_df)
```


Cross validation

```{r}
set.seed(56)
cross_vd =
  crossv_mc(child_df, 100) %>% 
  mutate(train = map(train, as_tibble),
         test = map(test, as_tibble))
```


```{r}
cross_validation = cross_vd %>% 
  mutate(main_effect_fit = map(train, ~ lm(bwt ~ blength + gaweeks, data = .x)),
         interation_fit = map(train, ~ lm(bwt ~ bhead * blength *babysex, data = .x)),
         proposed_fit = map(train, ~ lm(bwt ~ gaweeks + babysex + momage + mrace + frace + wtgain +  smoken, data = .x))) %>% 
  mutate(rmse_main_effect_fit = map2_dbl(main_effect_fit, test, ~ rmse(model = .x, data = .y)),
         rmse_interation_fit = map2_dbl(interation_fit, test, ~ rmse(model = .x, data = .y)),
         rmse_proposed_fit = map2_dbl(proposed_fit, test, ~ rmse(model = .x, data = .y)))
```


A violn plot comparing the rmse of the 3 models 

```{r}
cross_validation %>% 
  select(starts_with("rmse")) %>% 
  gather(key = model, value = rmse) %>% 
   mutate(model = str_replace(model, "rmse_", ""),
         model = fct_inorder(model)) %>% 
  ggplot(aes(x = model, y = rmse)) + geom_violin()
```



A table showing the mean_rmse of the 3 models

```{r}
cross_validation %>% 
  select(starts_with("rmse")) %>% 
  gather(key = model, value = rmse) %>% 
  group_by(model) %>% 
  summarize(mean_rmse = mean(rmse)) %>% 
  arrange(mean_rmse) %>% 
  knitr::kable(digits = 2)
```


The output from the graph and table shows that the best model is the model that contains the interaction term. My proposed fit was the worst model (sad)